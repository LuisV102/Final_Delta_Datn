import cv2
import numpy as np
import time
import uuid  # Thêm thư viện để tạo ID duy nhất

# --- Các hằng số và biến toàn cục của module ---
# MA TRẬN NỘI TẠI
CAMERA_MATRIX = np.array([
    [815.98760425, 0, 337.32019432],
    [0, 816.18913792, 241.58831339],
    [0, 0, 1]
], dtype=np.float32)
# HỆ SỐ MÉO
DIST_COEFFS = np.array([0.13648701, -0.15912073, -0.01095812, 0.00887588, -3.32255901], dtype=np.float32)

# CẤU HÌNH ROI (REGION OF INTEREST)
HEIGHT_ROI = [0, 480]  # ĐÂY LÀ TỌA ĐỘ GÓC TRÁI TRÊN
WIDTH_ROI = [100, 389]  # PHẢI DƯỚI
ROI_X1, ROI_Y1, ROI_X2, ROI_Y2 = WIDTH_ROI[0], HEIGHT_ROI[0], WIDTH_ROI[1], HEIGHT_ROI[1]

Y_TOP = 80
Y_TRIGGER = 200
Y_BOTTOM = 400
real_distance_trig_to_top = 39.5  # mm
real_distance_bottom_to_trig = 68  # mm

# VÙNG MÀU CẦN NHẬN DIỆN
COLOR_RANGES = {
    'red': ([0, 70, 50], [10, 255, 255], [160, 70, 50], [180, 255, 255]),
    'green': ([40, 70, 50], [80, 255, 255]),
    'yellow': ([25, 90, 95], [35, 250, 255]),
}
# KHI ĐẾN TRIGGER LINE THÌ NHẬN DIỆN VUÔNG XANH THÀNH TRÒN XANH
# Biến trạng thái cho việc nhận diện
_tracking_active = False
_start_time = 0  # THỜI GIAN XUẤT HIỆN LẦN ĐẦU
_start_y = 0
_max_velocity = 0.0  # Đảm bảo là float
_current_object_info = None
_object_color_detected = None
_last_command_time = 0
_command_sent = False  # ĐÃ GỬI LỆNH ĐIỀU KHIỂN CHƯA
_predicted_time_to_top = 0.0  # Đảm bảo là float
_predicted_time_robot_reach = 0.0  # Đảm bảo là float
_calibration_data_list = []
_command_cooldown_duration = 3.0  # Thời gian cooldown sau khi gửi lệnh (giây)
_current_object_id = None
_current_shape_detected = None
_is_auto_mode = False  # Khởi tạo chế độ (có thể thiết lập lại từ GUI)

# Ngưỡng diện tích tối thiểu cho contour được coi là hợp lệ
MIN_CONTOUR_AREA = 150

# Thêm biến toàn cục để theo dõi vật thể đã được đếm
_counted_objects = set()

# CÁC BIẾN MỚI CHO LOGIC VẬN TỐC
_initial_max_velocity_found = False
_velocity_samples = []
_AVERAGE_VELOCITY_SAMPLE_THRESHOLD = 5  # Số lượng mẫu vận tốc tối thiểu để bắt đầu tính trung bình
_VELOCITY_STABILIZATION_ZONE = 0.8  # Tỷ lệ quãng đường để xác định vận tốc ban đầu ổn định (80% quãng đường từ Y_BOTTOM đến Y_TRIGGER)

# LƯU BIẾN Ở DẠNG DICTIONARY (đây là dạng mảng liên kết, key - value)
_objects_memory = {
    'red_star': {'count': 0},
    'green_star': {'count': 0},
    'yellow_star': {'count': 0},
    'red_square': {'count': 0},
    'green_square': {'count': 0},
    'yellow_square': {'count': 0},
    'red_triangle': {'count': 0},
    'green_triangle': {'count': 0},
    'yellow_triangle': {'count': 0}
}


def set_operation_mode(is_auto):
    """
    Thiết lập chế độ hoạt động của hệ thống.
    Args:
        is_auto (bool): True nếu ở chế độ AUTO, False nếu ở chế độ MANUAL.
    """
    global _is_auto_mode
    _is_auto_mode = bool(is_auto)
    if _is_auto_mode:
        print("ObjectDetection: Chế độ AUTO - Gửi dữ liệu được phép.")
    else:
        print("ObjectDetection: Chế độ MANUAL - Không gửi dữ liệu.")


def get_object_memory():
    """Trả về bản sao của bộ nhớ vật thể để hiển thị trên GUI"""
    return _objects_memory.copy()


def reset_object_memory():
    """Reset tất cả bộ nhớ ĐẾM vật thể"""
    global _objects_memory, _counted_objects
    for key in _objects_memory.keys():
        _objects_memory[key] = {'count': 0}
    _counted_objects.clear()  # Đảm bảo reset cả set các ID đã đếm


def reset_detection_state():
    """Reset all state variables for object detection."""
    global _tracking_active, _start_time, _start_y, _max_velocity
    global _current_object_info, _object_color_detected, _last_command_time
    global _command_sent, _predicted_time_to_top, _predicted_time_robot_reach
    global _current_object_id, _current_shape_detected
    # Biến mới cho logic vận tốc
    global _initial_max_velocity_found, _velocity_samples

    _current_object_id = None
    _current_shape_detected = None
    _tracking_active = False
    _start_time = 0
    _start_y = 0
    _max_velocity = 0.0
    _current_object_info = None
    _object_color_detected = None
    _last_command_time = 0
    _command_sent = False
    _predicted_time_to_top = 0.0
    _predicted_time_robot_reach = 0.0

    _initial_max_velocity_found = False
    _velocity_samples = []  # Xóa các mẫu vận tốc


def process_frame_for_detection(input_frame, ser_instance):
    """
    Processes a single frame for object detection, drawing, and communication.
    Args:
        input_frame: The raw frame from the camera.
        ser_instance: The serial port instance for communication with Arduino.
    Returns:
        The processed frame with detections and information drawn on it.
    """
    global _tracking_active, _start_time, _start_y, _max_velocity
    global _current_object_info, _object_color_detected, _last_command_time
    global _command_sent, _predicted_time_to_top, _predicted_time_robot_reach
    global _current_object_id, _current_shape_detected
    # Biến mới cho logic vận tốc
    global _initial_max_velocity_found, _velocity_samples

    if input_frame is None:
        return None

    frame = input_frame.copy()  # TẠO BẢN SAO ĐỂ VẼ CÁC ĐƯỜNG ROI

    # Always draw ROI lines and text, regardless of detection state
    cv2.rectangle(frame, (ROI_X1, ROI_Y1), (ROI_X2, ROI_Y2), (0, 0, 255), 2)
    cv2.line(frame, (ROI_X1, Y_BOTTOM), (ROI_X2, Y_BOTTOM), (255, 0, 255), 2)
    cv2.putText(frame, "Bottom line", (ROI_X1 + 10, Y_BOTTOM - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
    cv2.line(frame, (ROI_X1, Y_TRIGGER), (ROI_X2, Y_TRIGGER), (0, 255, 255), 2)
    cv2.putText(frame, "Trigger line", (ROI_X1 + 10, Y_TRIGGER - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
    cv2.line(frame, (ROI_X1, Y_TOP), (ROI_X2, Y_TOP), (255, 0, 0), 2)
    cv2.putText(frame, "Top line", (ROI_X1 + 10, Y_TOP - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

    # --- Logic để tránh đơ camera sau khi gửi lệnh ---
    # Nếu đã gửi lệnh VÀ chưa đủ thời gian cooldown, thì chỉ vẽ khung và bỏ qua phần xử lý nhận diện vật thể
    if _command_sent and (time.time() - _last_command_time) < _command_cooldown_duration:
        return frame  # Trả về frame đã vẽ đường mà không thực hiện detect object

    # Nếu đã đủ thời gian cooldown, reset trạng thái để sẵn sàng nhận diện vật thể tiếp theo
    if _command_sent and (time.time() - _last_command_time) >= _command_cooldown_duration:
        reset_detection_state()  # Reset trạng thái để bắt đầu nhận diện vật thể mới

    roi = frame[ROI_Y1:ROI_Y2, ROI_X1:ROI_X2]
    # CĂT PHẦN ẢNH TRONG VÙNG ROI ĐỂ GIẢM DIỆN TÍCH XỬ LÝ
    if roi.size == 0:
        print("Warning: ROI is empty.")
        return frame

    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    # CHUYỂN ROI SANG HSV ĐỂ XỬ LÝ NHẬN DIỆN MÀU TỐT HƠN SO VỚI BGR

    # Biến để lưu trữ contour lớn nhất được tìm thấy trong khung hình hiện tại
    # và màu sắc của nó, để đảm bảo chỉ xử lý một vật thể chính
    best_contour = None  # GIẢM NHIỄU CHO HÌNH BAO VẬT THỂ LỚN NHẤT TÌM ĐƯỢC
    best_color_name = None

    # Tìm contour lớn nhất cho mỗi màu và chọn contour lớn nhất trong số đó
    for color_name, hsv_limits_tuple in COLOR_RANGES.items():  # Đổi tên biến 'ranges' cho rõ ràng
        mask = None  # Khởi tạo mask

        if len(hsv_limits_tuple) == 4:
            # Trường hợp có 2 dải màu (ví dụ: red)
            # hsv_limits_tuple là (lower1, upper1, lower2, upper2)
            lower1, upper1, lower2, upper2 = hsv_limits_tuple
            # mask1 tạo mặt nạ nhị phân cho dải màu thứ nhất
            mask1 = cv2.inRange(hsv, np.array(lower1), np.array(upper1))
            mask2 = cv2.inRange(hsv, np.array(lower2), np.array(upper2))
            mask = cv2.bitwise_or(mask1, mask2)
            # Bất kỳ pixel nào thuộc về dải màu thứ nhất HOẶC dải màu thứ hai
            # đều sẽ được bao gồm trong mặt nạ cuối cùng.
            # Các màu như 'red' có thể nằm ở hai đầu của thang đo Hue (0/180)
        elif len(hsv_limits_tuple) == 2:
            # Trường hợp có 1 dải màu (ví dụ: green, yellow)
            # hsv_limits_tuple là (lower, upper)
            lower, upper = hsv_limits_tuple
            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))
        else:
            print(f"Cảnh báo: Số lượng khoảng HSV không hợp lệ cho màu {color_name}. Bỏ qua màu này.")
            continue  # Bỏ qua màu này nếu cấu trúc không mong đợi

        if mask is None:  # Kiểm tra lại nếu mask chưa được tạo
            continue

        # LOẠI BỎ NHIỄU
        kernel = np.ones((5, 5), np.uint8)  # kernel này là hình vuông
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.medianBlur(mask, 5)

        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # Lọc các contour có diện tích nhỏ hơn ngưỡng và tìm contour lớn nhất cho màu hiện tại
        valid_contours = [cnt for cnt in contours if cv2.contourArea(cnt) >= MIN_CONTOUR_AREA]

        if valid_contours:
            current_largest_contour = max(valid_contours, key=cv2.contourArea)
            # So sánh với best_contour tổng thể để tìm ra vật thể lớn nhất trong tất cả các màu
            if best_contour is None or cv2.contourArea(current_largest_contour) > cv2.contourArea(best_contour):
                best_contour = current_largest_contour
                best_color_name = color_name
                _object_color_detected = 'Y' if best_color_name == 'yellow' else best_color_name[0].upper()

    # --- Chỉ xử lý contour lớn nhất được tìm thấy trong toàn bộ khung hình ---
    if best_contour is not None:
        cnt = best_contour
        color_name = best_color_name  # Gán lại color_name cho contour được chọn

        x, y, w, h = cv2.boundingRect(cnt)
        text_x_base = ROI_X1 + x
        # --- Phần tính toán màu sắc trung bình và vẽ ---
        # VẼ BOUNDING BOX VÀ TẠO MASK CHO CONTOUR
        mask_cnt = np.zeros(roi.shape[:2], dtype=np.uint8)
        cv2.drawContours(mask_cnt, [cnt], -1, 255, thickness=cv2.FILLED)
        mean_hsv_val = cv2.mean(hsv, mask=mask_cnt)
        # DÙNG HSV TRUNG BÌNH ĐỂ TÍNH LẠI MÀU SẮC
        hue_val = int(mean_hsv_val[0])
        sat_val = int(mean_hsv_val[1])
        brightness_val = int(mean_hsv_val[2])

        bgr_color_obj = cv2.cvtColor(np.uint8([[[hue_val, sat_val, brightness_val]]]), cv2.COLOR_HSV2BGR)[0][0]
        bgr_color_tuple = tuple(int(c) for c in bgr_color_obj)

        cv2.rectangle(frame, (ROI_X1 + x, ROI_Y1 + y), (ROI_X1 + x + w, ROI_Y1 + y + h), bgr_color_tuple, 2)

        M = cv2.moments(cnt)
        if M["m00"] != 0:
            cx_roi = int(M["m10"] / M["m00"])
            cy_roi = int(M["m01"] / M["m00"])
            # TÍNH TÂM CONTOUR THỰC TẾ
            cx_frame = ROI_X1 + cx_roi
            cy_frame = ROI_Y1 + cy_roi

            # Bỏ qua nếu tâm lệch (vẫn giữ logic này cho contour lớn nhất)
            bbox_cx_roi = x + w // 2
            bbox_cy_roi = y + h // 2
            distance = np.sqrt((cx_roi - bbox_cx_roi) ** 2 + (cy_roi - bbox_cy_roi) ** 2)
            max_allowed_distance = 0.4 * min(w, h)
            if distance > max_allowed_distance:
                # Nếu contour lớn nhất vẫn bị lệch tâm, không xử lý tiếp
                return frame

            cv2.circle(frame, (cx_frame, cy_frame), 6, (0, 0, 0), -1)
            # Vẽ tâm vật thể lên GUI

            # --- Hiệu chỉnh méo và chuyển đổi tọa độ ---
            distorted_point = np.array([[[cx_frame, cy_frame]]], dtype=np.float32)
            undistorted = cv2.undistortPoints(distorted_point, CAMERA_MATRIX, DIST_COEFFS, P=CAMERA_MATRIX)
            undistorted_coords = undistorted[0][0]

            Z_CONST = 263  # Chiều cao giả định của vật thể so với camera
            fx, fy = CAMERA_MATRIX[0, 0], CAMERA_MATRIX[1, 1]
            cx_intr, cy_intr = CAMERA_MATRIX[0, 2], CAMERA_MATRIX[1, 2]
            real_x = (undistorted_coords[0] - cx_intr) * Z_CONST / fx
            real_y = (undistorted_coords[1] - cy_intr) * Z_CONST / fy
            # tọa độ thật của vật thể trong hệ tọa độ camera
            # DÙNG CÔNG THỨC CAMERA PINHOLE ĐỂ CHUYỂN PIXEL QUA mm
            real_y_top_trig = (Y_TRIGGER - Y_TOP) * Z_CONST / fy
            P_CA = np.array([[real_x], [real_y], [Z_CONST], [1]])
            T_0C = np.array([
                [0, -1, 0, -160],
                [-1, 0, 0, -27],
                [0, 0, -1, -132],
                [0, 0, 0, 1]
            ])  # MA TRẬN CHUYỂN ĐỔI CAMERA QUA ROBOT
            P_OA = T_0C @ P_CA
            calib_x, calib_y, calib_z = P_OA[0, 0], P_OA[1, 0], P_OA[2, 0]
            calib_x_top = P_OA[0, 0] + real_y_top_trig

            if len(_calibration_data_list) < 20:  # _calibration_data_list không được reset trong reset_detection_state, nên cẩn thận với mục đích sử dụng.
                _calibration_data_list.append(calib_x)

            # --- Xử lý logic tracking và tính toán vận tốc ---
            current_y_on_frame = cy_roi + ROI_Y1

            # NHẬN DIỆN HÌNH DẠNG TRƯỚC KHI XỬ LÝ TRIGGER LINE
            epsilon = 0.02 * cv2.arcLength(cnt, True)
            approx = cv2.approxPolyDP(cnt, epsilon, True)
            vertices = len(approx)
            hull = cv2.convexHull(cnt)
            area = cv2.contourArea(cnt)
            hull_area = cv2.contourArea(hull)
            solidity = float(area) / hull_area if hull_area > 0 else 0

            (x, y, w, h) = cv2.boundingRect(cnt)
            aspect_ratio = float(w) / h if h != 0 else 1.0

            side_ratio = 0
            if vertices == 4:
                sides = []
                for i in range(4):
                    pt1 = approx[i][0]
                    pt2 = approx[(i + 1) % 4][0]
                    length = np.linalg.norm(pt1 - pt2)
                    sides.append(length)
                max_len = max(sides)
                min_len = min(sides)
                side_ratio = min_len / max_len if max_len != 0 else 0

            # ƯU TIÊN HÌNH CÓ 4 CẠNH TRƯỚC
            if vertices == 4 and 0.85 <= aspect_ratio <= 1.15 and side_ratio > 0.85 and solidity > 0.9:
                shape = "Square"
            elif vertices == 4 and solidity > 0.9:
                shape = "Rectangle"
            elif 6 <= vertices <= 9 and 0.9 <= aspect_ratio <= 1.1 and solidity > 0.93 and area > 1500:
                shape = "Square"  # Có thể là hình vuông được xấp xỉ với nhiều đỉnh
            elif 10 <= vertices <= 16 and solidity < 0.85:
                shape = "Star"
            elif vertices > 16 and solidity > 0.90:
                shape = "Circle"
            elif 3 <= vertices <= 6 and solidity > 0.85 and area > 300:
                shape = "Triangle"
            else:
                shape = "Unknown"
            _current_shape_detected = shape  # Cập nhật biến toàn cục

            # Hiển thị thông tin hình dạng và màu sắc
            cv2.putText(frame, f"{color_name} {shape}", (text_x_base, ROI_Y1 + y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, bgr_color_tuple, 2)

            # Bắt đầu tracking khi vật đi qua bottom line
            if current_y_on_frame >= Y_BOTTOM and not _tracking_active and not _command_sent:
                # Ghi nhận thời gian, vị trí bắt đầu, màu vật thể, gán ID bằng uuid
                _tracking_active = True
                _start_time = time.time()
                _start_y = current_y_on_frame
                _current_object_info = (cx_frame, cy_frame)
                # Đảm bảo _object_color_detected được gán khi bắt đầu tracking
                # if color_name == 'yellow':
                #     _object_color_detected = 'Y'
                # else:
                #     _object_color_detected = color_name[0].upper()
                _current_object_id = str(uuid.uuid4())
                print(
                    f"DEBUG: Bắt đầu tracking vật thể {color_name} {shape} tại Y_BOTTOM={Y_BOTTOM}, thời gian: {_start_time}")

            # Tính toán vận tốc khi vật đang di chuyển
            if _tracking_active and not _command_sent:
                current_time = time.time()
                elapsed_time = current_time - _start_time

                if elapsed_time > 0:
                    distance_pixels = abs(current_y_on_frame - _start_y)
                    distance_mm = distance_pixels * Z_CONST / fy
                    current_velocity = distance_mm / elapsed_time
                    # TÍNH VẬN TỐC HIỆN TẠI DỰA TRÊN QUÃNG ĐƯỜNG ĐI TỪ Y_BOTTOM
                    # print(f"DEBUG: current_y_on_frame: {current_y_on_frame}, current_velocity: {current_velocity:.2f} mm/s") # Bỏ comment để debug

                    # GIAI ĐOẠN 1: Xác định vận tốc tối đa ban đầu để loại bỏ vận tốc nhỏ
                    if not _initial_max_velocity_found:
                        if current_velocity > _max_velocity:
                            _max_velocity = current_velocity
                        # Xác định khi nào đã tìm thấy vận tốc tối đa ban đầu
                        # Vật đã đi được một phần quãng đường từ Y_BOTTOM và _max_velocity đã khác 0
                        stabilization_point_y = Y_BOTTOM - (Y_BOTTOM - Y_TRIGGER) * (1 - _VELOCITY_STABILIZATION_ZONE)
                        if current_y_on_frame <= stabilization_point_y and _max_velocity > 0:
                            _initial_max_velocity_found = True
                            print(
                                f"DEBUG: Đã xác định vận tốc tối đa ban đầu: {_max_velocity:.2f} mm/s tại Y={current_y_on_frame}")
                            # Reset danh sách mẫu vận tốc để chỉ thu thập sau khi vận tốc ban đầu ổn định
                            _velocity_samples = []

                    # GIAI ĐOẠN 2: Tính toán vận tốc trung bình sau khi vận tốc ban đầu đã ổn định
                    if _initial_max_velocity_found:
                        _velocity_samples.append(current_velocity)
                        # Giới hạn số lượng mẫu để tránh bộ nhớ tăng quá lớn và đảm bảo tính tức thời
                        if len(_velocity_samples) > 20:  # Giới hạn 20 mẫu gần nhất
                            _velocity_samples.pop(0)

                        # Tính vận tốc trung bình từ các mẫu
                        if len(_velocity_samples) >= _AVERAGE_VELOCITY_SAMPLE_THRESHOLD:
                            average_velocity = sum(_velocity_samples) / len(_velocity_samples)
                            # Sử dụng vận tốc trung bình này để dự đoán
                            # Đảm bảo vận tốc trung bình không quá nhỏ để tránh chia cho 0
                            effective_velocity = max(average_velocity, 0.1)  # Đặt một ngưỡng tối thiểu 0.1 mm/s

                            distance_to_top_mm = (Y_TRIGGER - Y_TOP) * Z_CONST / fy
                            _predicted_time_to_top = distance_to_top_mm / effective_velocity
                            _predicted_time_robot_reach = _predicted_time_to_top - 0.4  # Điều chỉnh thêm 0.4s
                            # print(f"DEBUG: Avg vel: {average_velocity:.2f}, Pred time to top: {_predicted_time_to_top:.2f}, Pred robot reach: {_predicted_time_robot_reach:.2f}") # Bỏ comment để debug

            # XỬ LÝ TRIGGER LINE
            # Cần đảm bảo calib_x_top được tính toán chính xác dựa trên tọa độ thực của vật thể
            # Việc cộng 25 trực tiếp vào calib_x_top ở đây có thể cần xem xét lại tùy theo mục đích
            # của calib_x_top (tọa độ đích của robot hay chỉ là offset).
            # Nếu nó là tọa độ đích, nên tính toán dựa trên các phép biến đổi không gian.
            # Nếu chỉ là offset, thì không cần thay đổi ở đây.
            # calib_x_top = calib_x_top + 25 # Có vẻ như dòng này nên nằm ngoài điều kiện if _tracking_active...
            # hoặc là một biến riêng biệt cho đích cuối cùng.

            if _tracking_active and current_y_on_frame <= Y_TRIGGER and not _command_sent:
                object_key = f"{color_name}_{shape.lower()}"  # Sử dụng shape đã được nhận diện

                # Kiểm tra vật thể chưa được đếm
                if _current_object_id is not None and object_key in _objects_memory:
                    # Tạo key duy nhất cho vật thể này
                    object_unique_key = f"{object_key}_{_current_object_id}"

                    if object_unique_key not in _counted_objects:
                        _objects_memory[object_key]['count'] += 1
                        _counted_objects.add(object_unique_key)
                        print(f"Đã phát hiện: {object_key}, Tổng số: {_objects_memory[object_key]['count']}")

                # Gửi lệnh đến Arduino CHỈ KHI Ở CHẾ ĐỘ AUTO
                if _is_auto_mode:
                    # Sử dụng _predicted_time_robot_reach đã được tính toán từ vận tốc trung bình
                    # Đảm bảo có một vận tốc hợp lệ đã được tính trước khi gửi lệnh
                    # và _predicted_time_robot_reach đã được cập nhật từ logic vận tốc trung bình
                    if ser_instance and ser_instance.is_open and _object_color_detected and _predicted_time_robot_reach > -0.3:  # Thêm ngưỡng âm nhỏ cho trường hợp sai số
                        if _predicted_time_robot_reach <= 0:
                            print(
                                "WARNING: _predicted_time_robot_reach vẫn bằng 0 hoặc âm (sau khi tính toán), gán lại 0.5s")
                            _predicted_time_robot_reach = 0.5  # Hoặc một giá trị default hợp lý khác

                        # Cập nhật calib_x_top ngay trước khi gửi lệnh nếu nó là tọa độ đích cuối cùng
                        # hoặc đảm bảo nó đã được tính toán đúng logic của bạn.
                        # Dòng này nên được đặt ở đây nếu calib_x_top chỉ mang ý nghĩa offset cho vị trí x.
                        # Nếu calib_x_top là một biến toàn cục được tính từ P_OA, thì nó đã có giá trị.
                        # Việc cộng 25 ở đây có thể đã được xử lý ở phần trên.
                        # calib_x_top_adjusted = calib_x_top + 25 # Nên tạo biến mới hoặc điều chỉnh logic calib_x_top ban đầu

                        command = f"Next:{calib_x_top:.1f},{calib_y:.1f},{calib_z:.1f};T:{_predicted_time_robot_reach:.2f};C:{_object_color_detected}\n"
                        try:
                            # ser_instance.write(command.encode()) # Bỏ comment khi chạy với Arduino thật
                            print(f"Sent to Arduino (AUTO): {command.strip()}")
                            _command_sent = True
                            reset_detection_state()  # Reset trạng thái ngay sau khi gửi lệnh thành công
                            _last_command_time = time.time()
                        except Exception as e:
                            print(f"Error sending to Arduino (AUTO): {e}")
                    else:
                        print(f"DEBUG: AUTO Mode: Không gửi lệnh. Điều kiện không thỏa mãn: "
                              f"Ser_instance: {bool(ser_instance)}, Ser_open: {ser_instance.is_open if ser_instance else 'N/A'}, "
                              f"Color_detected: {bool(_object_color_detected)}, Predicted_time_robot_reach: {_predicted_time_robot_reach:.2f} (phải > 0)")
                else:
                    # Ở chế độ MANUAL, không gửi lệnh, nhưng có thể log thông tin
                    if _object_color_detected and _current_shape_detected:  # Chỉ log nếu có vật thể để tránh spam
                        print(
                            f"MANUAL Mode: Object {color_name} {_current_shape_detected} at trigger. Data not sent. Coords: ({calib_x_top:.1f}, {calib_y:.1f}, {calib_z:.1f}), Predicted Time: {_predicted_time_robot_reach:.2f}")

                # Dù là AUTO hay MANUAL, khi vật thể chạm trigger, ta kết thúc tracking vật thể hiện tại.
                # reset_detection_state() đã được gọi ở trên khi gửi lệnh, hoặc ở đầu hàm nếu cooldown đã hết.
                # Do đó, các dòng dưới đây có thể không cần thiết nếu reset_detection_state() đủ toàn diện.
                # Tuy nhiên, nếu bạn muốn đảm bảo các biến này được reset ngay lập tức sau khi vật thể đi qua trigger (kể cả ở MANUAL),
                # thì bạn có thể giữ lại hoặc tinh chỉnh reset_detection_state().
                # Hiện tại, reset_detection_state() bao gồm tất cả các biến này, nên tôi sẽ bỏ comment các dòng riêng lẻ này
                # để tránh việc reset trùng lặp hoặc không nhất quán.

            # --- Hiển thị thông tin lên frame ---
            text_x_base = ROI_X1 + x
            text_y_base = ROI_Y1 + y + h

            robot_coords_text = f"Robot: ({calib_x:.1f}, {calib_y:.1f}, {calib_z:.1f})"
            cv2.putText(frame, robot_coords_text, (text_x_base, text_y_base + 60),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)

            # Hiển thị vận tốc dự đoán
            predicted_time_text = f"Pred. Time: {_predicted_time_robot_reach:.2f}s"
            cv2.putText(frame, predicted_time_text, (text_x_base, text_y_base + 80),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)

    return frame